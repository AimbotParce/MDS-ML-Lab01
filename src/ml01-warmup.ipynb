{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEefWC6UqDDC"
   },
   "source": [
    "# On data pre-processing\n",
    "\n",
    "Each problem requires a different approach in what concerns data cleaning and preparation.\n",
    "This pre-process can have a **deep impact on performance**; it can easily take a **significant amount of time**\n",
    "\n",
    "1. Attribute coding (discretization, encoding)\n",
    "2. Normalization (range, distribution)\n",
    "3. Missing values (imputation)\n",
    "4. Outliers\n",
    "5. Feature selection\n",
    "6. Feature extraction (feature engineering)\n",
    "7. Dimensionality reduction and transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1739439265493,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "Wy-fnJeqpl2S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q82ylJlNq6-v"
   },
   "source": [
    "## 1. Discretization / encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1739439715199,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "G6d6L1pVpl2X",
    "outputId": "e0fe9273-6eef-435d-808f-8dbb50d4eeac"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create a toy dataset\n",
    "data = {\n",
    "    'Category': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B'],\n",
    "    'Value': [12.3, 23.4, 45.6, 56.7, 67.8, 78.9, 89.0, 90.1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the initial dataset\n",
    "print(\"Initial Dataset:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Apply one-hot encoding to the categorical column\n",
    "df_encoded = pd.get_dummies(df, columns=['Category'], prefix=['Category'])\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 3: Discretize the continuous column into bins\n",
    "# Let's discretize the 'Value' column into 3 bins: Low, Medium, High\n",
    "df_encoded['Value_bin'] = pd.cut(df_encoded['Value'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Display the dataset after discretization\n",
    "print(\"\\nDataset after Discretization:\")\n",
    "print(df_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Drop the original 'Value' column if you only want to keep the discretized version\n",
    "df_encoded = df_encoded.drop(columns=['Value'])\n",
    "\n",
    "# Display the final dataset\n",
    "print(\"\\nFinal Dataset:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Step 1: Create a toy dataset with a continuous column\n",
    "data = {\n",
    "    'Feature1': [10, 20, 30, 40, 50],\n",
    "    'Feature2': [1, 2, 3, 4, 5],\n",
    "    'Feature3': [-1, 2, -3, -4, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the initial dataset\n",
    "print(\"Initial Dataset:\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Normalize the continuous column using Min-Max Scaling (to [0, 1])\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df['Feature1_minmax'] = min_max_scaler.fit_transform(df[['Feature1']])\n",
    "df['Feature2_minmax'] = min_max_scaler.fit_transform(df[['Feature2']])\n",
    "df['Feature3_minmax'] = min_max_scaler.fit_transform(df[['Feature3']])\n",
    "\n",
    "# Display the dataset after Min-Max normalization\n",
    "print(\"\\nDataset after Min-Max Normalization:\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Normalize the continuous column using Z-score Standardization (mean=0, std=1)\n",
    "standard_scaler = StandardScaler()\n",
    "df['Feature1_standard'] = standard_scaler.fit_transform(df[['Feature1']])\n",
    "df['Feature2_standard'] = standard_scaler.fit_transform(df[['Feature2']])\n",
    "df['Feature3_standard'] = standard_scaler.fit_transform(df[['Feature3']])\n",
    "\n",
    "# Display the dataset after Z-score standardization\n",
    "print(\"\\nDataset after Z-score Standardization:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imputation of missing values\n",
    "\n",
    "Imputing missing values using a model-based sophisticated method involves using machine learning models to predict and fill in the missing values. This approach is more advanced than simple mean/median/mode imputation and can capture relationships between features to provide more accurate imputations.\n",
    "\n",
    "A common model-based method is using k-Nearest Neighbors (k-NN) or regression models (e.g., linear regression, decision trees, or even more advanced models like Random Forests or Gradient Boosting). Below is an example using the KNNImputer from Scikit-learn, which is a popular model-based imputation method.\n",
    "\n",
    "Check out the page for more sophisticated imputers from scikit-learn's page https://scikit-learn.org/stable/modules/impute.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create a toy dataset with missing values\n",
    "data = {\n",
    "    'Feature1': [10, 20, np.nan, 40, 50],\n",
    "    'Feature2': [1, np.nan, 3, 4, 5],\n",
    "    'Feature3': [np.nan, 2, 3, np.nan, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the initial dataset with missing values\n",
    "print(\"Initial Dataset with Missing Values:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Impute missing values with the mean of the column\n",
    "df_mean_imputed = df.fillna(df.mean())\n",
    "\n",
    "# Display the dataset after mean imputation\n",
    "print(\"\\nDataset after Mean Imputation:\")\n",
    "print(df_mean_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Impute missing values with the median of the column\n",
    "df_median_imputed = df.fillna(df.median())\n",
    "\n",
    "# Display the dataset after median imputation\n",
    "print(\"\\nDataset after Median Imputation:\")\n",
    "print(df_median_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Impute missing values with a constant (e.g., 0)\n",
    "df_constant_imputed = df.fillna(0)\n",
    "\n",
    "# Display the dataset after constant imputation\n",
    "print(\"\\nDataset after Constant Imputation (0):\")\n",
    "print(df_constant_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Impute missing values with the mode of the column (most frequent value)\n",
    "df_mode_imputed = df.fillna(df.mode().iloc[0])\n",
    "\n",
    "# Display the dataset after mode imputation\n",
    "print(\"\\nDataset after Mode Imputation:\")\n",
    "print(df_mode_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses k-NN algorithm for imputation (we will see this in class when we do classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Step 1: Create a toy dataset with missing values\n",
    "data = {\n",
    "    'Feature1': [10, 20, np.nan, 40, 50],\n",
    "    'Feature2': [1, np.nan, 3, 4, 5],\n",
    "    'Feature3': [np.nan, 2, 3, np.nan, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the initial dataset with missing values\n",
    "print(\"Initial Dataset with Missing Values:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Use KNNImputer to impute missing values\n",
    "# The KNNImputer uses the k-nearest neighbors to fill in missing values\n",
    "imputer = KNNImputer(n_neighbors=1)  # Use 1 nearest neighbors\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Display the dataset after KNN imputation\n",
    "print(\"\\nDataset after KNN Imputation:\")\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier detection\n",
    "\n",
    "Finding outliers is an important step in data preprocessing for machine learning. Outliers are data points that deviate significantly from the rest of the data and can negatively impact the performance of machine learning models. There are several methods to detect outliers, including:\n",
    "\n",
    "1. Statistical Methods:\n",
    "\n",
    "    - __Z-score__: Identifies outliers based on how many standard deviations a data point is from the mean.\n",
    "    - __IQR__ (Interquartile Range): Identifies outliers based on the spread of the middle 50% of the data.\n",
    "\n",
    "2. Visual Methods:\n",
    "\n",
    "    - __Boxplots__: Visualize the distribution of features of the data.\n",
    "    - __Scatterplots__: Useful for identifying outliers in multivariate data.\n",
    "\n",
    "3. Model-Based Methods:\n",
    "\n",
    "    - __Isolation Forest__: An unsupervised algorithm that isolates outliers.\n",
    "    - __DBSCAN__: A clustering algorithm that can identify outliers as noise.\n",
    "\n",
    "Below is a Python script that demonstrates how to detect outliers using __Z-score__, and __IQR__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a toy dataset with potential outliers\n",
    "data = {\n",
    "    'Feature1': [10, 12, 14, 1000, 15, 13, 11, 12, 14, 13],\n",
    "    'Feature2': [1, 2, 3, 4, 5, 6, 7, 8, 9, 100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the initial dataset\n",
    "print(\"Initial Dataset:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Detect outliers using Z-score\n",
    "# Z-score measures how many standard deviations a data point is from the mean\n",
    "df['Z_score_Feature1'] = zscore(df['Feature1'])\n",
    "df['Z_score_Feature2'] = zscore(df['Feature2'])\n",
    "\n",
    "# Define outliers as data points with a Z-score greater than 3 or less than -3\n",
    "df['Outlier_Z_Feature1'] = np.abs(df['Z_score_Feature1']) >= 3\n",
    "df['Outlier_Z_Feature2'] = np.abs(df['Z_score_Feature2']) >= 3\n",
    "\n",
    "print(\"\\nDataset with Z-score Outlier Detection:\")\n",
    "print(df[['Feature1', 'Feature2', 'Z_score_Feature1', 'Outlier_Z_Feature1', 'Z_score_Feature2', 'Outlier_Z_Feature2']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Detect outliers using IQR (Interquartile Range)\n",
    "Q1 = df[['Feature1', 'Feature2']].quantile(0.25)\n",
    "Q3 = df[['Feature1', 'Feature2']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outliers as data points outside 1.5 * IQR from Q1 and Q3\n",
    "df['Outlier_IQR_Feature1'] = (df['Feature1'] < (Q1['Feature1'] - 1.5 * IQR['Feature1'])) | (df['Feature1'] > (Q3['Feature1'] + 1.5 * IQR['Feature1']))\n",
    "df['Outlier_IQR_Feature2'] = (df['Feature2'] < (Q1['Feature2'] - 1.5 * IQR['Feature2'])) | (df['Feature2'] > (Q3['Feature2'] + 1.5 * IQR['Feature2']))\n",
    "\n",
    "print(\"\\nDataset with IQR Outlier Detection:\")\n",
    "print(df[['Feature1', 'Feature2', 'Outlier_IQR_Feature1', 'Outlier_IQR_Feature2']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature selection\n",
    "\n",
    "We will cover this in more detail in a future lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature extraction (feature engineering)\n",
    "\n",
    "Feature engineering is the process of creating new features or transforming existing ones to improve the performance of machine learning models. \n",
    "Below is a Python exammple that creates a toy dataset where feature engineering can significantly improve model performance. \n",
    "The dataset simulates a scenario where raw features are not directly useful, but engineered features (e.g., interactions, transformations, or aggregations) can reveal meaningful patterns.\n",
    "\n",
    "In general, this is a very domain-specific task that requires human knowledge, so it is _hard to automate_.\n",
    "\n",
    "### Toy Dataset: Predicting House Prices\n",
    "\n",
    "In this example, we create a dataset where:\n",
    "\n",
    "- The target variable (`Price`) depends on __non-linear relationships__ and __interactions__ between features.\n",
    "- Raw features (`Size`, `Rooms`, `Age`) are not directly useful, but engineered features (e.g., `Size per Room`, `Age squared`) can improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a toy dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Raw features\n",
    "size = np.random.uniform(500, 3000, n_samples)  # Size of the house in sq. ft.\n",
    "rooms = np.random.randint(1, 6, n_samples)     # Number of rooms\n",
    "age = np.random.randint(0, 100, n_samples)      # Age of the house in years\n",
    "\n",
    "# Simulate the target variable (Price) with non-linear relationships and interactions\n",
    "price = (\n",
    "    100 * (size / rooms)  # Price increases with size per room\n",
    "    - 50 * (age ** 0.5)   # Price decreases with the square root of age\n",
    "    + np.random.normal(0, 5000, n_samples)  # Add some noise\n",
    ")\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Size': size,\n",
    "    'Rooms': rooms,\n",
    "    'Age': age,\n",
    "    'Price': price\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Raw Dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Feature Engineering\n",
    "# Create new features that capture non-linear relationships and interactions\n",
    "df['Size_per_Room'] = df['Size'] / df['Rooms']  # Size per room\n",
    "df['Age_squared'] = df['Age'] ** 0.5            # Square root of age\n",
    "df['Size_Age_Interaction'] = df['Size'] * df['Age_squared']  # Interaction between size and age\n",
    "\n",
    "# Display the dataset with engineered features\n",
    "print(\"\\nDataset with Engineered Features:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Compare raw vs engineered features\n",
    "# Check correlation of raw and engineered features with the target variable\n",
    "correlation = df.corr()['Price'].sort_values(ascending=False)\n",
    "print(\"\\nCorrelation with Target (Price):\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings from seaborn\n",
    "warnings.filterwarnings('ignore', module='seaborn')\n",
    "\n",
    "# Step 4: Plot pair-wise scatterplots to visually show correlations\n",
    "sns.pairplot(df, diag_kind='kde', height=1.5, aspect=1);\n",
    "\n",
    "# Add a title\n",
    "plt.suptitle(\"Pairwise Scatterplots of Features\", y=1.02);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimensionality reduction and transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a technique used to reduce the number of features in a dataset while preserving as much information as possible. \n",
    "\n",
    "__Principal Component Analysis (PCA)__ is one of the most popular dimensionality reduction techniques. Below is a Python script that creates a toy dataset and demonstrates how to apply PCA for dimensionality reduction.\n",
    "\n",
    "More on this and other __non-linear dimensionality reduction techniques__ in upcoming lab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Create a toy dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Generate random data for 2 features\n",
    "feature1 = np.random.normal(0, 1, n_samples)\n",
    "feature2 = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# Create a third feature as a linear combination of the first two\n",
    "feature3 = 0.5 * feature1 + 0.5 * feature2 + np.random.normal(0, 0.1, n_samples)  # Add some noise\n",
    "\n",
    "# Combine into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2,\n",
    "    'Feature3': feature3\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Toy Dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Standardize the data (PCA is sensitive to the scale of the features)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Step 3: Apply PCA to reduce the dataset to 2 dimensions\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Convert the result to a DataFrame for better visualization\n",
    "df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Display the first few rows of the transformed dataset\n",
    "print(\"\\nDataset after PCA:\")\n",
    "print(df_pca.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Visualize the original and reduced datasets\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the original dataset (3D)\n",
    "ax1 = plt.subplot(1, 2, 1, projection='3d')\n",
    "ax1.scatter(df['Feature1'], df['Feature2'], df['Feature3'])\n",
    "ax1.set_title(\"Original Dataset (3D)\")\n",
    "ax1.set_xlabel(\"Feature1\")\n",
    "ax1.set_ylabel(\"Feature2\")\n",
    "ax1.set_zlabel(\"Feature3\")\n",
    "\n",
    "# Plot the reduced dataset (2D)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(df_pca['PC1'], df_pca['PC2'])\n",
    "plt.title(\"Reduced Dataset (2D) after PCA\")\n",
    "plt.xlabel(\"Principal Component 1 (PC1)\")\n",
    "plt.ylabel(\"Principal Component 2 (PC2)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Explain the variance captured by each principal component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"\\nExplained Variance by Each Principal Component:\")\n",
    "for i, var in enumerate(explained_variance):\n",
    "    print(f\"PC{i+1}: {var:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "316px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "57623d20f10f61c2b585499338fbd94ecc6c3cd92aebb8069801a14d94a43cc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
